{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import os\n",
    "from scipy.signal import find_peaks, argrelextrema\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Read videos from the folder\n",
    "# Get the video file names\n",
    "\n",
    "video_files = os.listdir('videos')\n",
    "video_files = [file for file in video_files if file.endswith('.mp4')]\n",
    "video_files = [video_files[2], video_files[3]]\n",
    "print(video_files)\n",
    "\n",
    "for file in video_files:\n",
    "    video = VideoFileClip('videos/' + file)\n",
    "    duration = video.duration\n",
    "\n",
    "    middle_start = duration / 2 - 5\n",
    "    middle_end = duration / 2 + 5\n",
    "\n",
    "    middle_clip = video.subclip(middle_start, middle_end)\n",
    "\n",
    "    middle_clip.write_videofile(\"processed/\" + file, audio=False, codec=\"libx264\")\n",
    "\n",
    "\n",
    "\n",
    "caps = []\n",
    "for file in video_files:\n",
    "    cap = cv2.VideoCapture('processed/' + file)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video stream or file\" + file)\n",
    "    else:\n",
    "        caps.append(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_angle(p1, p2, p3):\n",
    "    '''\n",
    "    Calculate the angle between three points. As defined in report.\n",
    "    '''\n",
    "    a = np.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2)\n",
    "    b = np.sqrt((p2[0] - p3[0])**2 + (p2[1] - p3[1])**2)\n",
    "    c = np.sqrt((p3[0] - p1[0])**2 + (p3[1] - p1[1])**2)\n",
    "    angle = math.acos((b**2 + c**2 - a**2)/(2*b*c))\n",
    "    return np.degrees(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate lowest y value for each video by looking at local minima of ankle facing the camera\n",
    "# Filter out innaccuracies by adding condition that peaks should be separated by at least 10 frames\n",
    "# get the frame number of the lowest y value for each video\n",
    "\n",
    "video_frames = []\n",
    "video_side = []\n",
    "\n",
    "for cap in caps:\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    times = []\n",
    "    y_values = []\n",
    "    frame_numbers = []\n",
    "\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "\n",
    "        # Get middle frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, cap.get(cv2.CAP_PROP_FRAME_COUNT)/2)\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Check if left or right \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Check which ankle is closer to camera\n",
    "        try:\n",
    "            world_landmarks = results.pose_world_landmarks.landmark\n",
    "            left_ankle = [world_landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x, world_landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y, world_landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].z]\n",
    "            right_ankle = [world_landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x, world_landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y, world_landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].z]\n",
    "\n",
    "            if left_ankle[2] < right_ankle[2]:\n",
    "                video_side.append('left')\n",
    "            else:\n",
    "                video_side.append('right')\n",
    "\n",
    "        except:\n",
    "            print(\"Could not detect ankle\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "\n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                \n",
    "                world_landmarks = results.pose_world_landmarks.landmark\n",
    "\n",
    "                ankle = None\n",
    "\n",
    "                if video_side[-1] == 'left':\n",
    "                    ankle = [world_landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x, world_landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y, world_landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].z]\n",
    "                else:\n",
    "                    ankle = [world_landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x, world_landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y, world_landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].z]\n",
    "\n",
    "                # Get time and y value\n",
    "                time = cap.get(cv2.CAP_PROP_POS_FRAMES) / fps\n",
    "                times.append(time)\n",
    "                y_values.append(ankle[1])\n",
    "\n",
    "                frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "                frame_numbers.append(frame_number)\n",
    "\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    video_frames.append((frame_numbers, np.array(y_values)))\n",
    "\n",
    "\n",
    "# Plot the y values for each video on a separate plot\n",
    "\n",
    "for video in video_frames:\n",
    "    plt.plot(video[0], video[1])\n",
    "    plt.ylabel('Y-Coordinate')\n",
    "    plt.xlabel('Frame Number')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "lowest_y_values = []\n",
    "\n",
    "for video in video_frames:\n",
    "    # Find local minima in data, separated by at least 10 frames, ensure peak depth is at least 0.01\n",
    "    peaks, _ = find_peaks(-video[1], distance=10)\n",
    "    # Add peaks along with their frame numbers to a list\n",
    "    peaks_and_frames = []\n",
    "    for peak in peaks:\n",
    "        peaks_and_frames.append((video[0][peak], video[1][peak]))\n",
    "\n",
    "print(peaks_and_frames)\n",
    "                \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand robustness of model:\n",
    "\n",
    "o research the influence of the quality of the video on the robustness of the solution, we recorded predictions of our solution for different videos under multiple input quality configurations. For each video we varied the fps value, resolution, and duration. Specifically, the duration parameter is the number of seconds we use to find angles at the lowest pedal points. The fps and resolution parameters are simply a downsampling of the original quality of the input video.\n",
    "\n",
    "For each configuration we record the values of the angle at the lowest pedal point. To measure the influence of a reduction in quality we look at the standard deviation of the angles. A higher standard deviation for a configuration means that the angles used to make the recommendation are more spread out and thus our recommendation is less precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the frame number to get the lowest y value for each video\n",
    "# Calculate the angle between the hip, knee and the ankle for each frame\n",
    "# Only look at the frames where the ankle is at the lowest y value\n",
    "\n",
    "video_angles = []\n",
    "i = 0\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "    for cap in caps:\n",
    "        angles = []\n",
    "\n",
    "        for frame in peaks_and_frames:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame[0])\n",
    "\n",
    "            ret, image = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Could not read frame\")\n",
    "                break\n",
    "\n",
    "\n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Make detection\n",
    "            angle_results = pose.process(image)\n",
    "\n",
    "\n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            hip = None\n",
    "            knee = None\n",
    "            ankle = None\n",
    "\n",
    "            try:\n",
    "\n",
    "                world_landmarks = angle_results.pose_world_landmarks.landmark\n",
    "                if video_side[i] == 'left':\n",
    "                    hip = [world_landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, world_landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "                    knee = [world_landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, world_landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "                    ankle = [world_landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x, world_landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "                else:\n",
    "                    hip = [world_landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, world_landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "                    knee = [world_landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, world_landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "                    ankle = [world_landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x, world_landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "            except:\n",
    "                print(\"Could not detect landmarks\")\n",
    "                pass\n",
    "        \n",
    "\n",
    "            # Calculate angle\n",
    "            angle = _calculate_angle(hip, knee, ankle)\n",
    "            angles.append(angle)\n",
    "        \n",
    "        video_angles.append(angles)\n",
    "            \n",
    "        i += 1\n",
    "\n",
    "\n",
    "print(video_angles)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing\n",
    "\n",
    "# Filter out values that are far from median. This is to remove outliers\n",
    "processed_angles = []\n",
    "for video in video_angles:\n",
    "    median = np.median(video)\n",
    "    filtered = [x for x in video if abs(x - median) < 10]\n",
    "    processed_angles.append(filtered)\n",
    "\n",
    "print(processed_angles)\n",
    "\n",
    "# However, any one angle might still contain some noise. For example, we cannot observe the position at the true lowest point because that point might lie in between two different frames. \n",
    "# The mean of all the observed angles should be a good estimate of the angle at the true lowest point. We use this mean angle for our final recommendation.\n",
    "\n",
    "\n",
    "# Calculate the mean angle for each video\n",
    "mean_angles = []\n",
    "for video in processed_angles:\n",
    "    mean_angles.append(np.mean(video))\n",
    "\n",
    "print(mean_angles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb21bad9a86c0ece8327c80a8cd43dc1d603eb6bf4e20be127ea0707212b1fe8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
